{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E254-OD_nSRJ",
        "VAPxs1kunkMQ",
        "4Qcv358Cnqjp",
        "gPL0OJCGn2gR",
        "nGnzqIThuh8w"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Template"
      ],
      "metadata": {
        "id": "E254-OD_nSRJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHf1dAVKAcZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd2f3be-9913-4c16-b81a-f6b4223af223"
      },
      "source": [
        "env = gym.make('FrozenLake-v1')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6usoQHAmqh",
        "outputId": "530a5f0b-1160-4893-e71a-763b634da4e3"
      },
      "source": [
        "env.P[0][3] # Transition model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Su0h0AqQz",
        "outputId": "c911bccb-f81f-4185-dcc9-218a9776f595"
      },
      "source": [
        "env.observation_space.n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ68w5bpBScC",
        "outputId": "bbd645fb-6a7e-4f51-c26a-44583e62b350"
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "source": [
        "def play(env, policy, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcuDDx6rC5YE",
        "outputId": "93d08c69-0ed5-4a5f-a438-09cb501722e3"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 54)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdyjjtGZC9NX",
        "outputId": "306cd8f0-76f5-4a07-ddb3-d8a4fbd7f21b"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play(env, policy_1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt0VhyMuDasc",
        "outputId": "296797d4-48d8-4c78-9b18-19d8e7228938"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play(env, policy_2, False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp6qhRFJDxWR",
        "outputId": "ec87fd48-18c4-43f5-84b9-21d77723e84b"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play(env, policy_3, False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU8Q1qMxD6Po"
      },
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G427z17PEmjQ",
        "outputId": "2bde8b9f-95b0-479c-a79e-e0e218386fed"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play_multiple_times(env, policy_0, 1000)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 0/1000\n",
            "Average number of steps: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1bkhaFdDmj_",
        "outputId": "668bdb01-0be3-421e-c609-f18903f539c0"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play_multiple_times(env, policy_1, 1000)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 60/1000\n",
            "Average number of steps: 12.033333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZYhsb_VEtuR",
        "outputId": "c3ddbd44-88c3-44ad-bcf0-4eeee364ca1f"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play_multiple_times(env, policy_2, 1000)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 100/1000\n",
            "Average number of steps: 15.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvvHdMesEzTH",
        "outputId": "e5143243-e36f-4769-e676-db63b932c879"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play_multiple_times(env, policy_3, 1000)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 751/1000\n",
            "Average number of steps: 36.806924101198405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSomNpxJE5lP"
      },
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "        \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            # print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    \n",
        "    return v_values"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7IhqEOgGkQX",
        "outputId": "5d59f1c2-1376-449f-8db4-6ef2e907018e"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "v_values_0 = policy_evaluation(env, policy_0)\n",
        "print(v_values_0)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMjJKI3GGrsN",
        "outputId": "d455fdb4-4683-40b0-ca65-498518f38aa3"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "v_values_1 = policy_evaluation(env, policy_1)\n",
        "print(v_values_1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01904157 0.01519815 0.03161906 0.02371389 0.02538879 0.\n",
            " 0.06648515 0.         0.05924054 0.13822794 0.18999823 0.\n",
            " 0.         0.21152109 0.56684236 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-26M77nEfcV",
        "outputId": "7de87319-44ab-44dc-fae0-7a2b0be16503"
      },
      "source": [
        "np.all(v_values_1 >= v_values_0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l49O1N8QG0S2",
        "outputId": "9912df0d-f79d-4cf8-de6d-0d1f9f0c1523"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "v_values_2 = policy_evaluation(env, policy_2)\n",
        "print(v_values_2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.02889625 0.01951972 0.03616977 0.0271268  0.04790519 0.\n",
            " 0.07391985 0.         0.08288277 0.19339319 0.21022995 0.\n",
            " 0.         0.35153135 0.62684674 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22pRvreGE3Yt",
        "outputId": "0d556ec5-11f8-43f2-93eb-f61477753ff1"
      },
      "source": [
        "np.all(v_values_2 >= v_values_1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTYYFq6BEXDd",
        "outputId": "015f963f-a2cb-4129-b074-8f770bbba2e4"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "v_values_3 = policy_evaluation(env, policy_3)\n",
        "print(v_values_3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.06888666 0.06141097 0.07440714 0.05580443 0.09185068 0.\n",
            " 0.11220679 0.         0.14543323 0.24749485 0.29961611 0.\n",
            " 0.         0.37993438 0.63901935 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcEfU3NYE7xN",
        "outputId": "4e9d8d01-61a7-41ab-e89c-e03cf09505b0"
      },
      "source": [
        "np.all(v_values_3 >= v_values_2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh4akjMSHJBF"
      },
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9):\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    step = 0\n",
        "    \n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "            \n",
        "            # compute the q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                \n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            # select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "        \n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            # print(f'Converged at {i}-th iteration.')\n",
        "            step = i\n",
        "            break\n",
        "\n",
        "    return v_values, step"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8xAljw7VuMP"
      },
      "source": [
        "optimal_v_values, step = value_iteration(env, max_iters=500, gamma=0.9)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7g9VA3lV2WW",
        "outputId": "9e5b65bc-39b0-4ef9-994b-343f3e6ac855"
      },
      "source": [
        "optimal_v_values"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.06888615, 0.06141054, 0.07440682, 0.05580409, 0.09185022,\n",
              "       0.        , 0.11220663, 0.        , 0.14543286, 0.2474946 ,\n",
              "       0.29961593, 0.        , 0.        , 0.3799342 , 0.63901926,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0an7gaV39e"
      },
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            \n",
        "            q_values.append(q_value)\n",
        "        \n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "    \n",
        "    return policy"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TGCF4G7XErH"
      },
      "source": [
        "optimal_policy = policy_extraction(env, optimal_v_values, gamma=0.9)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-m4ZqWZXKqG",
        "outputId": "e75c9322-29bf-4f96-acf6-04327beb0727"
      },
      "source": [
        "optimal_policy\n",
        "play_multiple_times(env, optimal_policy, 1000)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 730/1000\n",
            "Average number of steps: 37.25890410958904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZY-vM9YXMRX",
        "outputId": "822ee5f5-90d0-4791-8920-084110e1c62f"
      },
      "source": [
        "play_multiple_times(env, policy_1, 1000)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 49/1000\n",
            "Average number of steps: 11.040816326530612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My code"
      ],
      "metadata": {
        "id": "dAx_Iji2nXfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Iteration"
      ],
      "metadata": {
        "id": "VAPxs1kunkMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, max_iters=500, gamma=0.9):\n",
        "  \"\"\"Policy Initialize\"\"\"\n",
        "  pi_values = np.zeros(env.observation_space.n)\n",
        "  step = 0  # number of steps to converge\n",
        "\n",
        "  for i in range(max_iters):\n",
        "    prev_pi_values = np.copy(pi_values)\n",
        "\n",
        "    \"\"\"Policy Evaluation\"\"\"\n",
        "    v_values = policy_evaluation(env, pi_values)\n",
        "\n",
        "    \"\"\"Policy Improvement\"\"\"\n",
        "    pi_values = policy_extraction(env, v_values)\n",
        "\n",
        "    # Check exit condition\n",
        "    if np.array_equal(pi_values, prev_pi_values):\n",
        "      # print(f'Action converged at {i}-th iteration.')\n",
        "      step = i\n",
        "      break\n",
        "\n",
        "  return pi_values, step"
      ],
      "metadata": {
        "id": "1khXEfc9WPbU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FrozenLake-v1\n"
      ],
      "metadata": {
        "id": "4Qcv358Cnqjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment\n",
        "env = gym.make('FrozenLake-v1')"
      ],
      "metadata": {
        "id": "9tyJF4tOn8-N"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frozenlake_value_iteration_runtime = []\n",
        "frozenlake_policy_iteration_runtime = []\n",
        "\n",
        "frozenlake_value_iteration_step = []\n",
        "frozenlake_policy_iteration_step = []"
      ],
      "metadata": {
        "id": "nY6fzpxOQe9z"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Value Iteration"
      ],
      "metadata": {
        "id": "mVSU_0qGoV9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(50):\n",
        "  start = time.time()\n",
        "\n",
        "  # Value Iteration\n",
        "  optimal_v_values, step = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "\n",
        "  # Policy Extraction\n",
        "  optimal_pi_values = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  # Test Policy\n",
        "  # play_multiple_times(env, optimal_pi_values, max_episodes=1000)\n",
        "\n",
        "  frozenlake_value_iteration_runtime.append(end - start)\n",
        "  frozenlake_value_iteration_step.append(step)"
      ],
      "metadata": {
        "id": "ZuIPQBwkoVhV"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy Iteration"
      ],
      "metadata": {
        "id": "ORYWgZa3obuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(50):\n",
        "  start = time.time()\n",
        "\n",
        "  # Policy Iteration\n",
        "  optimal_pi_values, step = policy_iteration(env, max_iters=500, gamma=0.9)\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  # Test Policy\n",
        "  # play_multiple_times(env, optimal_pi_values, max_episodes=1000)\n",
        "\n",
        "  frozenlake_policy_iteration_runtime.append(end - start)\n",
        "  frozenlake_policy_iteration_step.append(step)"
      ],
      "metadata": {
        "id": "sGLqpfIfod3c"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average runtime of Value Iteration: {np.average(frozenlake_value_iteration_runtime)} sec\")\n",
        "\n",
        "print(f\"Average runtime of Policy Iteration: {np.average(frozenlake_policy_iteration_runtime)} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNVMkqFaRT8k",
        "outputId": "3986dcf6-37bc-4a92-fcc0-ca7cecc7c4a7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average runtime of Value Iteration: 0.04885705471038818 sec\n",
            "Average runtime of Policy Iteration: 0.04803274631500244 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FrozenLake8x8-v1"
      ],
      "metadata": {
        "id": "gPL0OJCGn2gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake8x8-v1\")"
      ],
      "metadata": {
        "id": "IdC_DeLlu3P7"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frozenlake_8x8_value_iteration_runtime = []\n",
        "frozenlake_8x8_policy_iteration_runtime = []\n",
        "\n",
        "frozenlake_8x8_value_iteration_step = []\n",
        "frozenlake_8x8_policy_iteration_step = []"
      ],
      "metadata": {
        "id": "M_AREAcgu-g_"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Value Iteration"
      ],
      "metadata": {
        "id": "TILZ0YuHu-hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(50):\n",
        "  start = time.time()\n",
        "\n",
        "  # Value Iteration\n",
        "  optimal_v_values, step = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "\n",
        "  # Policy Extraction\n",
        "  optimal_pi_values = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  # Test Policy\n",
        "  # play_multiple_times(env, optimal_pi_values, max_episodes=1000)\n",
        "\n",
        "  frozenlake_8x8_value_iteration_runtime.append(end - start)\n",
        "  frozenlake_8x8_value_iteration_step.append(step)"
      ],
      "metadata": {
        "id": "cZT0Qd8Ju-hI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy Iteration"
      ],
      "metadata": {
        "id": "xO7bzsORu-hJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(50):\n",
        "  start = time.time()\n",
        "\n",
        "  # Policy Iteration\n",
        "  optimal_pi_values, step = policy_iteration(env, max_iters=500, gamma=0.9)\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  # Test Policy\n",
        "  # play_multiple_times(env, optimal_pi_values, max_episodes=1000)\n",
        "\n",
        "  frozenlake_8x8_policy_iteration_runtime.append(end - start)\n",
        "  frozenlake_8x8_policy_iteration_step.append(step)"
      ],
      "metadata": {
        "id": "VqKY2h0Nu-hJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average runtime of Value Iteration: {np.average(frozenlake_8x8_value_iteration_runtime)} sec\")\n",
        "\n",
        "print(f\"Average runtime of Policy Iteration: {np.average(frozenlake_8x8_policy_iteration_runtime)} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9275789e-35b5-44fc-b7a8-4892e89c51cc",
        "id": "iZSpOl1Fu-hJ"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average runtime of Value Iteration: 0.3126011323928833 sec\n",
            "Average runtime of Policy Iteration: 0.39844379901885985 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Taxi-v3"
      ],
      "metadata": {
        "id": "nGnzqIThuh8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v3')"
      ],
      "metadata": {
        "id": "j4qWTkwXuf9b"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_value_iteration_runtime = []\n",
        "taxi_policy_iteration_runtime = []\n",
        "\n",
        "taxi_value_iteration_step = []\n",
        "taxi_policy_iteration_step = []"
      ],
      "metadata": {
        "id": "qL-b4mrOulOZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Value Iteration"
      ],
      "metadata": {
        "id": "sTSkHFCiulOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(50):\n",
        "  start = time.time()\n",
        "\n",
        "  # Value Iteration\n",
        "  optimal_v_values, step = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "\n",
        "  # Policy Extraction\n",
        "  optimal_pi_values = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  # Test Policy\n",
        "  # play_multiple_times(env, optimal_pi_values, max_episodes=1000)\n",
        "\n",
        "  taxi_value_iteration_runtime.append(end - start)\n",
        "  taxi_value_iteration_step.append(step)"
      ],
      "metadata": {
        "id": "yqFptKuxulOf"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy Iteration"
      ],
      "metadata": {
        "id": "SAjAzOK7ulOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(50):\n",
        "  start = time.time()\n",
        "\n",
        "  # Policy Iteration\n",
        "  optimal_pi_values, step = policy_iteration(env, max_iters=500, gamma=0.9)\n",
        "\n",
        "  end = time.time()\n",
        "\n",
        "  # Test Policy\n",
        "  # play_multiple_times(env, optimal_pi_values, max_episodes=1000)\n",
        "\n",
        "  taxi_policy_iteration_runtime.append(end - start)\n",
        "  taxi_policy_iteration_step.append(step)"
      ],
      "metadata": {
        "id": "4QBkUQgYulOg"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average runtime of Value Iteration: {np.average(taxi_value_iteration_runtime)} sec\")\n",
        "\n",
        "print(f\"Average runtime of Policy Iteration: {np.average(taxi_policy_iteration_runtime)} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37acf371-2a06-4dac-d5c2-af2647738c48",
        "id": "94Xrb38zulOg"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average runtime of Value Iteration: 2.501711354255676 sec\n",
            "Average runtime of Policy Iteration: 4.593431344032288 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "aA6AaboX8RZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
      ],
      "metadata": {
        "id": "X-zy64_lM26H"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"Average Time (VI)\": [np.average(frozenlake_value_iteration_runtime), \n",
        "                          np.average(frozenlake_8x8_value_iteration_runtime),\n",
        "                          np.average(taxi_value_iteration_runtime)],\n",
        "        \n",
        "    \"Average Iteration (VI)\": [np.average(frozenlake_value_iteration_step), \n",
        "                          np.average(frozenlake_8x8_value_iteration_step),\n",
        "                          np.average(taxi_value_iteration_step)],\n",
        "\n",
        "    \"Average Time (PI)\": [np.average(frozenlake_policy_iteration_runtime), \n",
        "                          np.average(frozenlake_8x8_policy_iteration_runtime),\n",
        "                          np.average(taxi_policy_iteration_runtime)],\n",
        "        \n",
        "    \"Average Iteration (PI)\": [np.average(frozenlake_policy_iteration_step), \n",
        "                          np.average(frozenlake_8x8_policy_iteration_step),\n",
        "                          np.average(taxi_policy_iteration_step)],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data, index = ['FrozenLake-v1', 'FrozenLake8x8-v1', 'Taxi-v3'])\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)"
      ],
      "metadata": {
        "id": "aizsSssAAoyK"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trong các toy games, thuật toán Value Iteration (VI) cho ra kết quả nhanh hơn (trung bình thời gian 50 lần chạy) tại các game:\n",
        "  # 1. FrozenLake8x8-v1 \n",
        "  # 2. Taxi-v3\n",
        "# Đặc biệt, với env Taxi-v3, Value Iteration chạy nhanh hơn Policy Iteration (PI) một cách rõ rệt (~2 lần).\n",
        "\n",
        "print(df)\n",
        "\n",
        "# Theo lý thuyết, Policy Iteration có độ phức tạp thời gian cho từng vòng lặp (O(S^2)) nhỏ hơn so với Value Iteration (O(S^2 + A)).\n",
        "# Tuy nhiên, dựa trên kết quả thực nghiệm, PI mặc dù mất ít vòng lặp (iteration) để đi đến hội tụ hơn.\n",
        "# Nhưng lại mất nhiều thời gian hơn để cho ra kết quả so với VI.\n",
        "\n",
        "# Điều này có thể suy ra được rằng PI mặc dù mất ít vòng lặp hơn, nhưng lại tốn nhiều thời gian cho việc đánh giá từng chiến lược và\n",
        "# cải thiện các chiến lược đó. \n",
        "# Đặc biệt đối với các bài toán với action space lớn (Taxi-v3), PI có thể gặp khó khăn trong việc tìm kiếm policy tối ưu. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAGPnC4L8TaZ",
        "outputId": "f7ba3d9f-6eb9-40d5-9e48-70be5162ffc7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Average Time (VI)  Average Iteration (VI)  \\\n",
            "FrozenLake-v1              0.048857                    79.0   \n",
            "FrozenLake8x8-v1           0.312601                   117.0   \n",
            "Taxi-v3                    2.501711                   116.0   \n",
            "\n",
            "                  Average Time (PI)  Average Iteration (PI)  \n",
            "FrozenLake-v1              0.048033                     5.0  \n",
            "FrozenLake8x8-v1           0.398444                     9.0  \n",
            "Taxi-v3                    4.593431                    16.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Ol4hQHhNVLR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}